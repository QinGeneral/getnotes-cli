"""æ ¸å¿ƒä¸‹è½½é€»è¾‘ â€” åˆ†é¡µæ‹‰å–ç¬”è®°å¹¶ä¸‹è½½é™„ä»¶"""

import json
import logging
import os
import time
from datetime import datetime
from pathlib import Path

import httpx

from getnotes_cli.auth import AuthToken
from getnotes_cli.cache import CacheManager
from getnotes_cli.config import NOTES_API_URL, PAGE_SIZE, REQUEST_DELAY
from getnotes_cli.markdown import (
    get_file_extension,
    note_to_markdown,
    sanitize_filename,
)

logger = logging.getLogger(__name__)


class NoteDownloader:
    """ç¬”è®°ä¸‹è½½å™¨"""

    def __init__(
        self,
        token: AuthToken,
        output_dir: Path,
        *,
        limit: int | None = 100,
        page_size: int = PAGE_SIZE,
        delay: float = REQUEST_DELAY,
        force: bool = False,
        save_json: bool = False,
    ):
        self.token = token
        self.output_dir = output_dir
        self.limit = limit
        self.page_size = page_size
        self.delay = delay
        self.force = force
        self.save_json = save_json

        self.client = httpx.Client(timeout=60)
        self.cache = CacheManager(output_dir)
        self.stats = {"new": 0, "updated": 0, "cached": 0}
        self.total_processed = 0

    def run(self) -> dict:
        """æ‰§è¡Œä¸‹è½½æµç¨‹ï¼Œè¿”å›ç»Ÿè®¡ç»“æœ"""
        # åˆ›å»ºç›®å½•ç»“æ„
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / "notes").mkdir(exist_ok=True)
        if self.save_json:
            (self.output_dir / "api_responses").mkdir(exist_ok=True)

        # åŠ è½½ç¼“å­˜
        if not self.force:
            self.cache.load()
            # ç¼“å­˜ä¸ºç©ºä½†æœ¬åœ°æœ‰ç¬”è®°æ–‡ä»¶å¤¹æ—¶ï¼Œè‡ªåŠ¨é‡å»ºç¼“å­˜
            notes_dir = self.output_dir / "notes"
            if self.cache.count == 0 and notes_dir.exists() and any(notes_dir.iterdir()):
                self.cache.rebuild_from_disk(notes_dir)

        self._print_banner()

        since_id = ""
        page_num = 0
        total_items = None

        try:
            while True:
                page_num += 1
                logger.info("ğŸ“„ æ­£åœ¨æ‹‰å–ç¬¬ %d é¡µ (since_id=%s) ...",
                            page_num, since_id or "(é¦–é¡µ)")

                data = self._fetch_page(since_id)

                # ä¿å­˜ API å“åº”
                if self.save_json:
                    resp_path = self.output_dir / "api_responses" / f"page_{page_num:04d}.json"
                    resp_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

                content = data.get("c", {})
                notes = content.get("list", [])
                has_more = content.get("has_more", False)

                if total_items is None:
                    total_items = content.get("total_items", "?")
                    logger.info("ğŸ“Š æœåŠ¡ç«¯ç¬”è®°æ€»æ•°: %s", total_items)

                if not notes:
                    logger.info("âš ï¸  æœ¬é¡µæ— æ•°æ®ï¼Œç»“æŸã€‚")
                    break

                logger.info("  æœ¬é¡µ %d æ¡ç¬”è®°:", len(notes))

                for note in notes:
                    self._process_note(note)
                    self.total_processed += 1

                    # å®šæœŸä¿å­˜ç¼“å­˜
                    if self.total_processed % 50 == 0:
                        self.cache.save()

                    # æ£€æŸ¥é™åˆ¶
                    if self.limit is not None and self.total_processed >= self.limit:
                        logger.info("âœ… å·²è¾¾åˆ°ä¸‹è½½é™åˆ¶ (%d æ¡)ï¼Œåœæ­¢ã€‚", self.limit)
                        logger.info("ğŸ’¡ æç¤º: è‹¥è¦ä¸‹è½½æ‰€æœ‰ç¬”è®°ï¼Œè¯·ä½¿ç”¨ `getnotes download --all`")
                        break

                if self.limit is not None and self.total_processed >= self.limit:
                    break

                if not has_more:
                    logger.info("âœ… æ‰€æœ‰ç¬”è®°å·²ä¸‹è½½å®Œæ¯•ï¼")
                    break

                since_id = notes[-1].get("id", "")
                time.sleep(self.delay)

        except KeyboardInterrupt:
            logger.info("âš ï¸  ç”¨æˆ·ä¸­æ–­ä¸‹è½½ã€‚")
        except httpx.HTTPStatusError as e:
            logger.error("âŒ HTTP é”™è¯¯: %s", e)
            logger.info("ğŸ’¡ å¯èƒ½éœ€è¦é‡æ–°ç™»å½•: getnotes login")
        except Exception as e:
            logger.error("âŒ æ„å¤–é”™è¯¯: %s", e)
            raise
        finally:
            self.cache.save()

        # ç”Ÿæˆç´¢å¼•
        self._generate_index(total_items)
        self._print_summary()

        return self.stats

    def _fetch_page(self, since_id: str = "") -> dict:
        """æ‹‰å–ä¸€é¡µç¬”è®°"""
        params = {
            "limit": self.page_size,
            "since_id": since_id,
            "sort": "create_desc",
        }
        headers = self.token.get_headers()
        resp = self.client.get(NOTES_API_URL, headers=headers, params=params, timeout=30)
        resp.raise_for_status()
        return resp.json()

    def _process_note(self, note: dict) -> str:
        """å¤„ç†å•æ¡ç¬”è®°"""
        note_id = note.get("note_id", note.get("id", "unknown"))
        title = note.get("title", "").strip()

        # ç¼“å­˜æ£€æŸ¥
        if not self.force and self.cache.is_cached(note):
            self._print_status(note_id, title, "â­ ç¼“å­˜")
            self.stats["cached"] += 1
            return "cached"

        is_update = self.cache.get(note_id) is not None

        # ç”Ÿæˆæ–‡ä»¶å¤¹å
        folder_name = self._make_folder_name(note)
        if is_update:
            cached = self.cache.get(note_id)
            if cached and "folder_name" in cached:
                folder_name = cached["folder_name"]

        note_dir = self.output_dir / "notes" / folder_name
        if not is_update and note_dir.exists():
            note_dir = self.output_dir / "notes" / f"{folder_name}_{note_id[-6:]}"
            folder_name = f"{folder_name}_{note_id[-6:]}"

        note_dir.mkdir(parents=True, exist_ok=True)
        attachments_dir = note_dir / "attachments"

        # ä¸‹è½½é™„ä»¶
        has_attachments = self._download_attachments(note, attachments_dir)

        # ç”Ÿæˆ Markdown
        md_content = note_to_markdown(note, attachments_dir, note_dir)
        (note_dir / "note.md").write_text(md_content, encoding="utf-8")

        # ä¿å­˜ JSONï¼ˆsave-json æ¨¡å¼ï¼‰
        if self.save_json:
            (note_dir / "note.json").write_text(
                json.dumps(note, ensure_ascii=False, indent=2), encoding="utf-8"
            )

        # æ›´æ–°ç¼“å­˜
        self.cache.update(note_id, {
            "version": note.get("version"),
            "updated_at": note.get("updated_at", ""),
            "folder_name": folder_name,
            "title": title,
            "created_at": note.get("created_at", ""),
        })

        action = "ğŸ”„ æ›´æ–°" if is_update else "âœ¨ æ–°å¢"
        extra = " ğŸ“" if has_attachments else ""
        self._print_status(note_id, title, f"{action}{extra}")
        self.stats["updated" if is_update else "new"] += 1
        return "updated" if is_update else "new"

    def _download_attachments(self, note: dict, attachments_dir: Path) -> bool:
        """ä¸‹è½½ç¬”è®°çš„é™„ä»¶ï¼ˆéŸ³é¢‘/å›¾ç‰‡ï¼‰ï¼Œè¿”å›æ˜¯å¦æœ‰é™„ä»¶"""
        has = False

        # éŸ³é¢‘é™„ä»¶
        for i, att in enumerate(note.get("attachments", [])):
            att_url = att.get("url", "")
            att_type = att.get("type", "")
            if att_url and att_type != "link":
                has = True
                ext = get_file_extension(att_url, f".{att_type or 'bin'}")
                self._download_file(att_url, attachments_dir / f"attachment_{i + 1}{ext}")

        # å›¾ç‰‡
        images = note.get("original_images", []) or note.get("small_images", []) or note.get("body_images", [])
        for i, img in enumerate(images):
            img_url = img if isinstance(img, str) else img.get("url", "") if isinstance(img, dict) else ""
            if img_url:
                has = True
                ext = get_file_extension(img_url, ".jpg")
                self._download_file(img_url, attachments_dir / f"image_{i + 1}{ext}")

        return has

    def _download_file(self, url: str, save_path: Path) -> bool:
        """ä¸‹è½½æ–‡ä»¶ï¼Œå·²å­˜åœ¨åˆ™è·³è¿‡"""
        try:
            if save_path.exists() and not self.force:
                kb = save_path.stat().st_size / 1024
                logger.info("    â­  å·²å­˜åœ¨: %s (%.1f KB)", save_path.name, kb)
                return True

            save_path.parent.mkdir(parents=True, exist_ok=True)
            with self.client.stream("GET", url, timeout=60) as resp:
                resp.raise_for_status()
                with open(save_path, "wb") as f:
                    for chunk in resp.iter_bytes(8192):
                        f.write(chunk)

            kb = save_path.stat().st_size / 1024
            logger.info("    âœ… å·²ä¸‹è½½: %s (%.1f KB)", save_path.name, kb)
            return True
        except Exception as e:
            logger.error("    âŒ ä¸‹è½½å¤±è´¥: %s - %s", save_path.name, e)
            return False

    def _make_folder_name(self, note: dict) -> str:
        """ç”Ÿæˆç¬”è®°æ–‡ä»¶å¤¹å"""
        note_id = note.get("note_id", note.get("id", "unknown"))
        title = note.get("title", "").strip()
        created_at = note.get("created_at", "")

        date_prefix = ""
        if created_at:
            try:
                dt = datetime.strptime(created_at, "%Y-%m-%d %H:%M:%S")
                date_prefix = dt.strftime("%Y%m%d_%H%M%S")
            except ValueError:
                date_prefix = created_at.replace(" ", "_").replace(":", "")

        return sanitize_filename(f"{date_prefix}_{title}" if title else f"{date_prefix}_{note_id}")

    def _print_status(self, note_id: str, title: str, action: str) -> None:
        """æ‰“å°ç¬”è®°å¤„ç†çŠ¶æ€"""
        parts = [f"#{self.total_processed + 1}", f"ID:{note_id[-8:]}"]
        if title:
            display = title[:40] + "..." if len(title) > 40 else title
            parts.append(display)
        parts.append(action)
        logger.info("  ğŸ“ %s", " | ".join(parts))

    def _print_banner(self) -> None:
        """æ‰“å°å¯åŠ¨ä¿¡æ¯"""
        logger.info("=" * 60)
        logger.info("ğŸ—‚ï¸  GetNotes CLI â€” å¾—åˆ°ç¬”è®°æ‰¹é‡ä¸‹è½½å·¥å…·")
        logger.info("=" * 60)
        mode = "å…¨éƒ¨ç¬”è®°" if self.limit is None else f"å‰ {self.limit} æ¡"
        logger.info("ğŸ“¥ æ¨¡å¼: %s", mode)
        logger.info("ğŸ“‚ è¾“å‡ºç›®å½•: %s", self.output_dir.resolve())
        logger.info("ğŸ“„ æ¯é¡µ: %d æ¡ | â±ï¸ é—´éš”: %ss", self.page_size, self.delay)
        if self.save_json:
            logger.info("ğŸ“ ä¿å­˜æ¨¡å¼: åŒ…å«åŸå§‹ JSON æ•°æ®")
        if self.force:
            logger.info("âš¡ å¼ºåˆ¶æ¨¡å¼: å¿½ç•¥æ‰€æœ‰ç¼“å­˜")
        elif self.cache.count > 0:
            logger.info("ğŸ’¾ ç¼“å­˜: å·²æœ‰ %d æ¡ç¬”è®°è®°å½•", self.cache.count)
        logger.info("=" * 60)

    def _print_summary(self) -> None:
        """æ‰“å°ä¸‹è½½æ€»ç»“"""
        logger.info("=" * 60)
        logger.info("ğŸ“Š ä¸‹è½½æ€»ç»“")
        logger.info("=" * 60)
        logger.info("  ğŸ“‹ å¤„ç†ç¬”è®°:   %d æ¡", self.total_processed)
        logger.info("  âœ¨ æ–°å¢:       %d æ¡", self.stats['new'])
        logger.info("  ğŸ”„ æ›´æ–°:       %d æ¡", self.stats['updated'])
        logger.info("  â­  ç¼“å­˜è·³è¿‡:   %d æ¡", self.stats['cached'])
        logger.info("  ğŸ’¾ ç¼“å­˜è®°å½•:   %d æ¡", self.cache.count)
        logger.info("  ğŸ“ è¾“å‡ºç›®å½•:   %s", self.output_dir.resolve())
        logger.info("=" * 60)

    def _generate_index(self, total_items) -> None:
        """ç”Ÿæˆç´¢å¼•æ–‡ä»¶"""
        index_path = self.output_dir / "INDEX.md"
        with open(index_path, "w", encoding="utf-8") as f:
            f.write("# Getç¬”è®° å¯¼å‡ºç´¢å¼•\n\n")
            f.write(f"- å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"- å¤„ç†ç¬”è®°æ•°: {self.total_processed}\n")
            f.write(f"- æœåŠ¡ç«¯æ€»æ•°: {total_items}\n")
            f.write(f"- æ–°å¢: {self.stats['new']} | æ›´æ–°: {self.stats['updated']} | ç¼“å­˜è·³è¿‡: {self.stats['cached']}\n\n")
            f.write("## ç¬”è®°åˆ—è¡¨\n\n")
            f.write("| # | ç¬”è®° ID | æ–‡ä»¶å¤¹ |\n")
            f.write("|---|---------|--------|\n")

            notes_dir = self.output_dir / "notes"
            if notes_dir.exists():
                for i, folder in enumerate(sorted(notes_dir.iterdir())):
                    if folder.is_dir():
                        json_file = folder / "note.json"
                        md_file = folder / "note.md"
                        if json_file.exists():
                            try:
                                nd = json.loads(json_file.read_text(encoding="utf-8"))
                                nid = nd.get("note_id", "")
                                title = nd.get("title", "(æ— æ ‡é¢˜)")
                                f.write(f"| {i + 1} | `{nid}` | [{title}](notes/{folder.name}/note.md) |\n")
                            except Exception:
                                f.write(f"| {i + 1} | - | [{folder.name}](notes/{folder.name}/note.md) |\n")
                        elif md_file.exists():
                            f.write(f"| {i + 1} | - | [{folder.name}](notes/{folder.name}/note.md) |\n")
